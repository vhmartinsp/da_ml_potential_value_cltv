# -*- coding: utf-8 -*-
"""da_ml_potential_value_cltv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqRG5FqlMEiKVfygTX1c8KW_2VHPHJ1z

## 1\. Contexto

A FLO deseja definir um roteiro para as atividades de vendas e marketing.
Para que a empresa faça um plano de médio e longo prazo, é necessário estimar o valor potencial que os clientes existentes fornecerão à empresa no futuro.

O conjunto de dados consiste em informações obtidas do comportamento de compras anteriores dos clientes que fizeram suas últimas compras no OmniChannel (on-line e off-line) entre 2020 e 2021.
"""

!pip install lifetimes
import numpy as np
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sns
from lifetimes import BetaGeoFitter
from lifetimes.plotting import plot_period_transactions
from lifetimes import GammaGammaFitter

"""
## 2\. Carregamento"""

#df = pd.read_csv('https://raw.githubusercontent.com/vhmartinsp/datasets/main/flo_data_20k.csv?token=GHSAT0AAAAAACQICVKGOX2PYSEBHRJPHYE4ZQGDGZA')

# Carregar o dataset
df = pd.read_csv('/content/flo_data_20k.csv')

"""
## 3\. Exploração dos Dados (EDA)"""

def eda(dataframe):
    print(f"""

    -- Info --

    {dataframe.dtypes}

    -- NaN Values --

    {dataframe.isnull().sum()}

    -- Shape --

    {dataframe.shape}

      -- Unique --

    {df.apply(lambda x: x.nunique())}

    -- Head --
    """)


    return dataframe.head()
eda(df)

# Função para realizar a Análise Exploratória de Dados (EDA)
def exploratory_data_analysis(dataframe):

    print("-- Informações --\n")
    print(dataframe.info())
    print("\n-- Valores Ausentes --\n")
    print(dataframe.isnull().sum())
    print("\n-- Formato --\n")
    print(dataframe.shape)
    print("\n-- Registros Únicos --\n")
    print(dataframe.nunique())
    print("\n-- Primeiros Registros --\n")
    print(dataframe.head())

exploratory_data_analysis(df)

from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42) # Aqui estaos definindo que 20% da base será para teste

# Junta o número de vendas online e offline para treino
train_data["order_num_total"] = train_data["order_num_total_ever_online"] + train_data["order_num_total_ever_offline"]
train_data["customer_value_total"] = train_data["customer_value_total_ever_offline"] + train_data["customer_value_total_ever_online"]
train_data = train_data[~((train_data["customer_value_total"] == 0) | (train_data["order_num_total"] == 0))] # Retorna True para todas as linhas onde o valor da coluna "customer_value_total" é diferente de zero

# Junta o número de vendas online e offline para teste
test_data
test_data["order_num_total"] = test_data["order_num_total_ever_online"] + test_data["order_num_total_ever_offline"]
test_data["customer_value_total"] = test_data["customer_value_total_ever_offline"] + test_data["customer_value_total_ever_online"]
test_data = test_data[~((test_data["customer_value_total"] == 0) | (test_data["order_num_total"] == 0))] # Retorna True para todas as linhas onde o valor da coluna "customer_value_total" é diferente de zero

unique_values = df['interested_in_categories_12'].unique() # Verificar únicas categorias existentes
unique_values

# Para tornar o Dataset um pouco mais real, vou substituir as categorias por categorias verdadeiras reais com o Dicionário de substituições
replacements_train = {
    '[KADIN]': 'Dresses',
    '[ERKEK, COCUK, KADIN, AKTIFSPOR]': 'Women\'s Shirts, Sneakers, Dresses, Sandals and Flats',
    '[ERKEK, KADIN]': 'Women\'s Shirts, Dresses',
    '[AKTIFCOCUK, COCUK]': 'Socks, Sneakers',
    '[AKTIFSPOR]': 'Sandals and Flats',
    '[COCUK]': 'Sneakers',
    '[ERKEK, COCUK, KADIN]': 'Women\'s Shirts, Sneakers, Dresses',
    '[KADIN, AKTIFSPOR]': 'Dresses, Sandals and Flats',
    '[AKTIFCOCUK, COCUK, KADIN]': 'Socks, Sneakers, Dresses',
    '[COCUK, KADIN, AKTIFSPOR]': 'Sneakers, Dresses, Sandals and Flats',
    '[ERKEK, AKTIFSPOR]': 'Socks, Sandals and Flats',
    '[AKTIFCOCUK, COCUK, AKTIFSPOR]': 'Socks, Sneakers, Sandals and Flats',
    '[ERKEK]': 'Women\'s Shirts',
    '[COCUK, KADIN]': 'Sneakers, Dresses',
    '[ERKEK, KADIN, AKTIFSPOR]': 'Women\'s Shirts, Dresses, Sandals and Flats',
    '[AKTIFCOCUK]': 'Socks',
    '[COCUK, AKTIFSPOR]': 'Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, KADIN, AKTIFSPOR]': 'Socks, Women\'s Shirts, Dresses, Sandals and Flats',
    '[ERKEK, COCUK]': 'Women\'s Shirts, Sneakers',
    '[Outras Categorias]': 'Others',
    '[AKTIFCOCUK, COCUK, KADIN, AKTIFSPOR]': 'Socks, Sneakers, Dresses, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK]': 'Socks, Women\'s Shirts, Sneakers',
    '[AKTIFCOCUK, ERKEK]': 'Socks, Women\'s Shirts',
    '[AKTIFCOCUK, KADIN]': 'Socks, Dresses',
    '[AKTIFCOCUK, ERKEK, COCUK, KADIN, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK, KADIN]': 'Socks, Women\'s Shirts, Sneakers',
    '[AKTIFCOCUK, AKTIFSPOR]': 'Socks, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, KADIN, AKTIFSPOR]': 'Socks, Dresses, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sandals and Flats',
    '[ERKEK, COCUK, AKTIFSPOR]': 'Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, KADIN]': 'Socks, Women\'s Shirts, Dresses',
    '[]': 'Not informed'
}


# Substituindo os valores no dataset
train_data['interested_in_categories_12'] = train_data['interested_in_categories_12'].replace(replacements_train)

replacements_test = {
    '[KADIN]': 'Dresses',
    '[ERKEK, COCUK, KADIN, AKTIFSPOR]': 'Women\'s Shirts, Sneakers, Dresses, Sandals and Flats',
    '[ERKEK, KADIN]': 'Women\'s Shirts, Dresses',
    '[AKTIFCOCUK, COCUK]': 'Socks, Sneakers',
    '[AKTIFSPOR]': 'Sandals and Flats',
    '[COCUK]': 'Sneakers',
    '[ERKEK, COCUK, KADIN]': 'Women\'s Shirts, Sneakers, Dresses',
    '[KADIN, AKTIFSPOR]': 'Dresses, Sandals and Flats',
    '[AKTIFCOCUK, COCUK, KADIN]': 'Socks, Sneakers, Dresses',
    '[COCUK, KADIN, AKTIFSPOR]': 'Sneakers, Dresses, Sandals and Flats',
    '[ERKEK, AKTIFSPOR]': 'Socks, Sandals and Flats',
    '[AKTIFCOCUK, COCUK, AKTIFSPOR]': 'Socks, Sneakers, Sandals and Flats',
    '[ERKEK]': 'Women\'s Shirts',
    '[COCUK, KADIN]': 'Sneakers, Dresses',
    '[ERKEK, KADIN, AKTIFSPOR]': 'Women\'s Shirts, Dresses, Sandals and Flats',
    '[AKTIFCOCUK]': 'Socks',
    '[COCUK, AKTIFSPOR]': 'Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, KADIN, AKTIFSPOR]': 'Socks, Women\'s Shirts, Dresses, Sandals and Flats',
    '[ERKEK, COCUK]': 'Women\'s Shirts, Sneakers',
    '[Outras Categorias]': 'Others',
    '[AKTIFCOCUK, COCUK, KADIN, AKTIFSPOR]': 'Socks, Sneakers, Dresses, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK]': 'Socks, Women\'s Shirts, Sneakers',
    '[AKTIFCOCUK, ERKEK]': 'Socks, Women\'s Shirts',
    '[AKTIFCOCUK, KADIN]': 'Socks, Dresses',
    '[AKTIFCOCUK, ERKEK, COCUK, KADIN, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK, KADIN]': 'Socks, Women\'s Shirts, Sneakers',
    '[AKTIFCOCUK, AKTIFSPOR]': 'Socks, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, COCUK, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, KADIN, AKTIFSPOR]': 'Socks, Dresses, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, AKTIFSPOR]': 'Socks, Women\'s Shirts, Sandals and Flats',
    '[ERKEK, COCUK, AKTIFSPOR]': 'Women\'s Shirts, Sneakers, Sandals and Flats',
    '[AKTIFCOCUK, ERKEK, KADIN]': 'Socks, Women\'s Shirts, Dresses',
    '[]': 'Not informed'
}

# Substituindo os valores no dataset
test_data['interested_in_categories_12'] = test_data['interested_in_categories_12'].replace(replacements_test)

train_data[train_data.columns[train_data.columns.str.contains("date")]].dtypes # Seleciona colunas que tenham "data" no nome

date_columns = train_data.columns[train_data.columns.str.contains("date")] # Retorna uma lista que satisfaça a condição anterior
train_data[date_columns] = train_data[date_columns].apply(pd.to_datetime) #Converte as datas para datetime

# Repete o processo para teste
date_columns = test_data.columns[test_data.columns.str.contains("date")]
test_data[date_columns] = test_data[date_columns].apply(pd.to_datetime)

columns = ['order_channel',] # Gráfico sobre os canais de venda
plt.figure(figsize=(18,14))
x, y = 1,3
for i,column in enumerate(train_data[columns]):
    plt.subplot(y, x, i+1)
    sns.countplot(x=df[column], palette='viridis')
    plt.xlabel('')
    plt.title('Order Channel')
    plt.xticks(rotation = 70)
plt.show()

"""

*   O aplicativo se apresenta como o maior canal de vendas, significando que nossos esforços deverão estar mais presentes nesse canal (push, inapp, notificações)


"""

columns = ['last_order_channel'] # Gráfico sobre o último canal de venda
plt.figure(figsize=(18,14))
x, y = 1,3
for i,column in enumerate(train_data[columns]):
    plt.subplot(y, x, i+1)
    sns.countplot(x=train_data[column], palette='viridis')
    plt.xlabel('')
    plt.title('Last order channel')
    plt.xticks(rotation = 70)
plt.show()

"""

*   Percebemos aqui que o aplicativo para Android apresenta ligeira vantagem sobre Offline (provavelmente pro conta do aplicativo de Android ser o maior canal de vendas), seguido por Mobile. Neste caso, podemos ver que mais pessoas pedem seu último pedido via canal Offline, cujo não é um dos maiores canais de venda, sendo assim devemos pensar no porquê esse canal está sendo ressaltado no gráfico

*   Há uma alta taxa de churn por conta da experiência Offline?



"""

columns = ['interested_in_categories_12'] #Gráfico das principais categorias compradas
plt.figure(figsize=(24, 14))
x, y = 1, 3

for i, column in enumerate(train_data[columns]):
    plt.subplot(y, x, i+1)
    sns.countplot(x=train_data[column], palette='viridis')
    plt.title('Distribuition of most shopped categories in the last 12 months')
    plt.xlabel('')
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')  # Ajusta a rotação e a posição dos rótulos no eixo x

plt.tight_layout()  # Ajusta o espaçamento entre os subplots
plt.show()

"""

*   Vemos aqui que Sandálias e Sapatilhas são destaques quando compradas juntas
*   Depois temos a categoria de vestidos, categoria não informada e camisetas femininhas

* Será que Sandálias e Sapatilhas são compradas em conjunto por alguma promoção? Há cross-sell quando compradas juntas da mesma marca? São compradas em uma estação específica?
*  A categoria não informada é algum erro de categorização ?




"""

train_data['first_order_year_month'] = train_data['first_order_date'].dt.to_period('M')
train_data['last_order_year_month'] = train_data['last_order_date'].dt.to_period('M')

first_order_counts = train_data.groupby('first_order_year_month').size()
last_order_counts = train_data.groupby('last_order_year_month').size()

order_counts = pd.concat([first_order_counts, last_order_counts], axis=1)
order_counts.columns = ['First Order', 'Last Order']

order_counts.plot(kind='line', figsize=(10, 6))
plt.title('Distribuição de Pedidos ao Longo do Tempo')
plt.xlabel('Ano e Mês')
plt.ylabel('Número de Pedidos')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""

*   Vemos que tivemos um grande pico de primeiras compras, aproximadamente 900, depois tivemos uma queda pela metade e uma recuperção no terceiro e quarto trimestre, depois tivemos uma queda que foi piorando.
*  Teria sido a pandemia que influenciou essa grande queda de primeiras compras ao piorar a economia e o poder aquisitivo?


*   Após osegundo trimestre de 2020 tivemos bem mais últimas compras do que primeiras compras, indicando que estamos tendo pessoas que estão deixando de comprar
*   Podemos pensar em alguma forma de recuperar essas pessoas ?



"""

train_data['last_order_date_online_year_month'] = train_data['last_order_date_online'].dt.to_period('M')
train_data['last_order_date_offline_year_month'] = train_data['last_order_date_offline'].dt.to_period('M')

last_order_online = train_data.groupby('last_order_date_online_year_month').size()
last_order_offline = train_data.groupby('last_order_date_offline_year_month').size()

last_order_omni = pd.concat([last_order_online, last_order_offline], axis=1)
last_order_omni.columns = ['Order Online', 'Order Offline']

last_order_omni.plot(kind='line', figsize=(10, 6))
plt.title('Distribuição de ambiente ao Longo do Tempo')
plt.xlabel('Ano')
plt.ylabel('Número de Pedidos')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""*   Dado que já estamos com pessoas parando de fazer sua primeira compra, também podemos ver que a última compra destas tende a ser no Smartphones, principalmente por conta da Pandemia

## 4\. Preparação dos Dados
"""

# Aqui vamos definir o quartil inferior e superior

def outlier_thresholds(dataframe, variable):
    quartile1 = dataframe[variable].quantile(0.01)
    quartile3 = dataframe[variable].quantile(0.99)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

# Substituirá outliers pelo valor do limite inferior ou superior
def replace_with_thresholds(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = round(low_limit,0)
    dataframe.loc[(dataframe[variable] > up_limit), variable] = round(up_limit,0)

columns = ["order_num_total_ever_online", "order_num_total_ever_offline", "customer_value_total_ever_offline","customer_value_total_ever_online"]
for col in columns:
    replace_with_thresholds(df, col)

train_data["last_order_date"].max()

analysis_date = dt.datetime(2021,6,30)

# Selecionar as colunas relevantes para a análise
train_data_final = pd.DataFrame()
train_data_final["customer_id"] = train_data["master_id"]
train_data_final["recency_cltv_weekly"] = ((train_data["last_order_date"]- train_data["first_order_date"]).astype('timedelta64[D]')) / 7
train_data_final["T_weekly"] = ((analysis_date - train_data["first_order_date"]).astype('timedelta64[D]'))/7
train_data_final["frequency"] = train_data["order_num_total"]
train_data_final["monetary_cltv_avg"] = train_data["customer_value_total"] / train_data["order_num_total"]

train_data_final.head()

# Repete o processo para os dados de teste, neste caso precisamos tratar os dados de teste para que as colunas sejam equivalentes aos dados de treino
test_data_final = pd.DataFrame()
test_data_final["customer_id"] = test_data["master_id"]
test_data_final["recency_cltv_weekly"] = ((test_data["last_order_date"]- test_data["first_order_date"]).astype('timedelta64[D]')) / 7
test_data_final["T_weekly"] = ((analysis_date - test_data["first_order_date"]).astype('timedelta64[D]'))/7
test_data_final["frequency"] = test_data["order_num_total"]
test_data_final["monetary_cltv_avg"] = test_data["customer_value_total"] / test_data["order_num_total"]

test_data_final.head()

"""## 5\. Modelagem BG-NBD

## Treino
"""

bgf_train = BetaGeoFitter(penalizer_coef=0.001)
bgf_train.fit(train_data_final['frequency'], train_data_final['recency_cltv_weekly'], train_data_final['T_weekly'])

# Previsão das compras futuras no conjunto de teste
test_data_final["exp_sales_1_month"] = bgf_train.predict(4 * 1, test_data_final['frequency'], # Transforma o número de meses em quantidade semana
                                                                test_data_final['recency_cltv_weekly'],
                                                                test_data_final['T_weekly'])

test_data_final["exp_sales_3_month"] = bgf_train.predict(4 * 3, test_data_final['frequency'],
                                                                test_data_final['recency_cltv_weekly'],
                                                                test_data_final['T_weekly'])

"""Vamos ver quem são as 10 pessoas que mais compraram

## Teste
"""

test_data_final.sort_values("exp_sales_1_month",ascending=False)[:10]

"""## 6\. Modelagem Gamma Gamma

## Treino
"""

ggf = GammaGammaFitter(penalizer_coef=0.01)
ggf.fit(train_data_final['frequency'], train_data_final['monetary_cltv_avg'])

"""## Teste"""

test_data_final["exp_average_value"] = ggf.conditional_expected_average_profit(test_data_final['frequency'],
                                                                               test_data_final['monetary_cltv_avg'])

"""Podemos calcular o o valor médio esperado para o tempo de 6 meses"""

cltv_df = ggf.customer_lifetime_value(bgf_train,
                                   test_data_final['frequency'],
                                   test_data_final['recency_cltv_weekly'],
                                   test_data_final['T_weekly'],
                                   test_data_final['monetary_cltv_avg'],
                                   time=6,  # Tempo em meses
                                   freq="W",  # Frequência dos dados (semanal)
                                   discount_rate=0.01)  # Taxa de desconto
cltv_df['cltv'] = cltv_df
cltv_df.head()

# Adiciona os valores de CLTV ao DataFrame
test_data_final["cltv"] = cltv.values

test_data_final.head()

"""Vamos ver quem são as pessoas com os 15 maiores CLTV"""

test_data_final.sort_values("cltv",ascending=False)[:15]
